#RL #position-paper

![[Pasted image 20250414233727.png]]
- motivation
	- general superhuman capabilities are only possible for an AI system if
		- it is able to pursue long-term goals (continuous learning)
		- it is grounded in experience through the agent, not just text books and human guidance
	- LLMs are great, but inherently unstable, i.e. they can hallucinate
	- RL took a backseat in the last years, but to break through the ceiling of human knowledge and skills, it will likely be a crucial part
- interaction with the real world
	- text input and output <- LLMs
	- computer interaction <- we are starting to be here
	- autonomous interactions with the real world <- the future
- reward
	- currently, LLMs optimize solely for human-based prejudgement -> neither the AI nor in most cases the human actually experience the "true" reward, it's just what a human thinks the reward is
	- in the future, rewards could and should also come from external events and signals
		- these rewards can still be determined by humans, *after* the fact, e.g. a human could report how tasty the cake was that was baked from the LLM recipe
	- where do rewards come from, if not from humans directly?
		- "cost, error rates, hunger, productivity, health metrics, climate metrics, profit, sales, exam results, success, visits, yields, stocks, likes, income, pleasure/pain, economic indicators, accuracy, power, distance, speed, efficiency, or energy consumption"
		- more rewards arise when specific events occur (and are observed)
	- overarching goal: small amount of human data -> allowing for lots of RL learning
- without grounding in real-world experience, any agent "will become an echo chamber of existing human knowledge"

🗓️ 2025

✍️
- [[David Silver]]
- [[Richard S. Sutton]]
