#LLM #VLM #RL #survey

- motivation
	- there's powerful foundation models (FMs), especially LLMs and VLM
	- can RL benefit from their world knowledge and reasoning capabilities?
	- let's see what people are trying and provide a taxonomy of recent methods
- taxonomy
	- LLM/VLM as agent: using FM directly as policy
		- parametric: fine-tuned FM
		- non-parametric: FM as-is
	- LLM/VLM as planner
		- comprehensive: more efficient, but riskier in dynamic settings
		- incremental: enables real-time feedback, but comes with overhead
	- LLM/VLM as reward
		- reward model: FM *is* the reward
		- reward function: FM *shapes* the reward

üóìÔ∏è 2025

‚úçÔ∏è
- [[Sheila Schoepp]]
- [[Masoud Jafaripour]]
- [[Yingyue Cao]]
- [[Tianpei Yang]]
- [[Fatemeh Abdollahi]]
- [[Shadan Golestan]]
- [[Zahin Sufiyan]]
- [[Osmar R. Zaiane]]
- [[Matthew E. Taylor]]
