#LLM

![[Pasted image 20250530234642.png]]
- motivation
	- Transformer architecture is crucial for foundation models
	- problem: inefficiency on long sequences
- introduced method: *Mamba*
	- fast inference (5x more throughput than Transformers)
	- linear scaling in sequence length
	- performance improves on real data up to million-length sequences

ğŸ—“ï¸ 2024

âœï¸
- [[Albert Gu]]
- [[Tri Dao]]
