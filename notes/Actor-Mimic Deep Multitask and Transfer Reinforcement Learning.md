#MTRL #transfer-learning #DL #actor-mimic #DQN #ALE #model-compression

- motivation
	- DQN can learn any Atari game using one architecture in a single-task setup
	- many tasks have similarities, e.g. pong and breakout, this can be exploited for better generalization => let's do MTRL
- introduced method: *Actor Mimic*
	- uses techniques from model compression to train a single multi-task model
	- source games $S_1,\dots S_N$ with corresponding game-specific expert DQNs $E_1,\dots,E_N$
		- name comes from the actor (=multi-task policy) mimicking the expert actions
	- one way to let experts guide multitask policy is to minimize squared loss between Q-values -> didn't work well for the authors
	- Actor-Mimic instead matches policies by first transforming Q-values using a softmax
		- policy regression objective => telling student how they should act
			- each expert DQN is transformed into a policy network by a Boltzmann distribution over the Q-value outputs: $$\pi_{E_i}(a\mid s)=\frac{e^{\tau{-1}}Q_{E_i}(s,a)}{\sum_{a'\in\mathcal A_{E_i}}e^{\tau^{-1}}Q_{E_i}(s,a')}$$ with temperature $\tau$
			- policy objective over multitask network is defined as cross-entropy loss $$L^i_{policy}(\theta)=\sum_{a\in\mathcal A_{E_i}}\pi_{E_i}(a\mid s)\log \pi_\text{AMN}(a\mid s;\theta)$$ where $\pi_\text{AMN}(a\mid s;\theta)$ is the multitask Actor-Mimic Network policy
				- in words: $L^i_{policy}$ measures how well the AMN mimics the expert's action distribution in state $s$
			- expert network output gives a stable supervised learning signal; this is unlike the recursively defined Q-learning objective which relies on itself as target
			- where to get training data from?
				- either sample from expert network or AMN action outputs
				- both work, but sampling from AMN performed better empirically
		- feature regression objective => teaching student why they should act that way
			- nudge AMN to compute features that can predict an expert's features
			- Let $h_\text{AMN}(s)$ be the hidden activations in the feature layer of the AMN
			- Let $h_{E_{i}}(s)$ be the hidden activations in the feature layer of the $i$th expert
				- not necessarily same dimensionality as $h_\text{AMS}$
			- Let $f_i(h_\text{AMN}(s))$ be a feature regression network, that, given state $s$, predicts the features $h_{E_i}(s)$ from $h_\text{AMN}(s)$
			- $L^i_{FeatureRegression}(\theta,\theta_{f_i})=\lVert f_i(h_\text{AMN}(s;\theta);\theta_{f_i})-h_{E_i(s)}\rVert_2^2$
		- putting it together
			- combined loss = policy regression objective + feature regression objective
			- $\mathcal L^i_{ActorMimic}(\theta,\theta_{f_i})=\mathcal L^i_{policy}(\theta)+\beta\mathcal L^i_{FeatureRegression}(\theta,\theta_{f_i})$
- math
	- we can extend the policy regression objective to the expectation over all states $$\min_\theta\mathbb E_{s\sim D^{{}^\pi\text{AMN},\epsilon\text{-greedy}}(\cdot)}\left[\mathcal H\left(\pi_E(a\mid s),\pi_\text{AMN}(a\mid s;\theta)\right)\right]+\lambda\lVert\theta\rVert_2^2$$
		- $\mathcal H$ is cross entropy measure
		- weight decay is needed for the following analysis
	- some more definitions
		- let $\Gamma: Q\to \pi_{\epsilon\text{-greedy}}$ be a mapping from Q function to $\epsilon$-greedy policy
		- assume: each state in MDP is represented by a $K$-dimensional feature representation $\phi(s)\in\mathbb R^K$ 
		- let $P_\theta$ be a $\lvert\mathcal S\rvert\times\lvert\mathcal A\rvert$ matrix where the element in row $i$ and column $j$ is the softmax policy prediction $p(a_j\mid s_i;\theta)$ from the linear approximator
		- let $\Pi_E$ be a $\lvert\mathcal S\rvert\times\lvert\mathcal A\rvert$ matrix, now for the expert predictions
		- let $D_\pi$ be a diagonal matrix with entries $D_\pi(s)$
		- let $Z_t^{\pi'}(s,\pi)$ be the $t$-step reward of executing $\pi$ in the initial state $s$ and then following policy $\pi'$
		- cost-to-go for a policy $\pi$ after $T$ steps: $J_T(\pi)=-T\mathbb E_{s\sim D(\cdot)}[\mathcal R(s,a)]$
	- according to the stochastic approximation argument (Robbins & Monro, 1951):
		- Under a fixed policy $œÄ^‚àó$ and a learning rate schedule that satisfies $\sum_{t=1}^\infty \alpha_t,\sum_{t=1}^\infty \alpha_t^2$, the parameters $\theta$, updated using $\nabla\theta_t=-\alpha_t\left[\Phi^TD_\pi(P_{\theta_{t-1}}-\Pi_E)+\lambda\theta_{t-1}\right]$, asymptotically almost surely converge to a unique solution $\theta^*$
	- convergence theorem:
		- assume the MDP is irreducible and aperiodic for any policy $\pi$ induced by $\Gamma$ and $\Gamma$ is Lipschitz continuous with a constant $c_\epsilon$, then the sequence of policies $\{\pi^1,\pi^2,\dots\}$ and model parameters $\{\theta^1,\theta^2,\dots\}$ generated by the iterative algorithm above converges almost surely to a unique solution $\pi^*$ and $\theta^*$
	- proposition: performance guarantee bound on how well AMN performs w.r.t. expert
		- if loss function converges to $\epsilon$ with the solution $\pi_\text{AMN}$ and $Z^{\pi^*}_{T-t+1}(s,\pi^*)-Z^{\pi^*}_{T-t+1}(s,a)\geq u$ for all actions $a\in\mathcal A$ and $t\in\{1,\dots, T\}$, then the cost-to-go of Actor-Mimic $J_T(\pi_\text{AMN})$ grows linearly after executing $T$ actions: $J_T(\pi_\text{AMN})\leq J_T(\pi_E)+\frac{uT\epsilon}{\log 2}$
- results
	- experiment setup 1: use Actor-Mimic to learn tasks simultaneously
	- experiment setup 2: use Actor-Mimic to pretrain makes learning new tasks faster
		- transfer learning setting, showing that source task representations generalized
		- this is done by removing the final softmax layer of the AMN and then using the weights of the AMN as an instantiation for a DQN
		- this setting benefits especially from the feature regression objective

üóìÔ∏è 2015

‚úçÔ∏è
- [[Emilio Parisotto]]
- [[Jimmy Ba]]
- [[Ruslan Salakhutdinov]]
