#gradient-descent

- motivation
	- saddle points supposedly pose a problem for gradient descent, leading to sub-optimal solutions
	- but how much of a problem are there really?
- findings of this work
	- while in the worst-case, convergence to saddle points can happen, gradient descent with random init and reasonable constant step size does not converge to a saddle point

ğŸ—“ï¸ 2016

âœï¸
- [[Jason D. Lee]]
- [[Max Simchowitz]]
- [[Michael I. Jordan]]
- [[Benjamin Recht]]
